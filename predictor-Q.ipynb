{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem Categories: 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [10:03<00:00,  6.03s/it]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, learning_rate, gamma, epsilon, state_bins, action_bins):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.state_bins = state_bins\n",
    "        self.action_bins = action_bins\n",
    "        self.q_table = np.zeros(rating_bins + [2] * num_problem_categories + [len(duration_bins)] + [action_bins])\n",
    "        self.duration_bins = None\n",
    "\n",
    "    def discretize_state(self, state):\n",
    "        rating_bins_edges = [0, 1000, 1200, 1400, 1600, 1800, 2000, 2200, 2400, 2700, 3000, 3001]\n",
    "        rating_idx = np.digitize(state[0], bins=rating_bins_edges) - 1\n",
    "        \n",
    "        # Adjust to handle tags separated by both semicolons and commas\n",
    "        problem_categories = [0] * num_problem_categories\n",
    "        tags_in_state = [tag.strip() for sublist in state[1].split(';') for tag in sublist.split(',') if tag.strip()]\n",
    "        for tag in tags_in_state:\n",
    "            if tag in limited_tags:\n",
    "                problem_categories[limited_tags.index(tag)] = 1\n",
    "        \n",
    "        duration_idx = np.digitize(state[2], bins=self.duration_bins) - 1\n",
    "        return tuple([rating_idx] + problem_categories + [duration_idx])\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.uniform(0, 1) < self.epsilon:\n",
    "            return np.random.choice(self.action_bins)\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state])\n",
    "\n",
    "    def update(self, state, action_value, reward, next_state, done):\n",
    "        # Map action value to index\n",
    "        action_index = action_to_index[action_value]\n",
    "        future_optimal_value = np.max(self.q_table[next_state])\n",
    "        learned_value = reward + self.gamma * future_optimal_value\n",
    "        old_value = self.q_table[state][action_index]\n",
    "        self.q_table[state][action_index] += self.learning_rate * (learned_value - old_value)\n",
    "\n",
    "def get_next_contest_state(data, episode):\n",
    "    contest_data = data.iloc[episode]\n",
    "    state = (\n",
    "        contest_data['Old Rating'],\n",
    "        contest_data['Problem Tags'],  # Extract as a string\n",
    "        contest_data['Contest Duration']\n",
    "    )\n",
    "    return state\n",
    "\n",
    "def simulate_next_contest(data, episode, action):\n",
    "    current_rating = data.iloc[episode]['Old Rating']\n",
    "    predicted_next_rating = current_rating + action\n",
    "    actual_next_rating = data.iloc[episode]['New Rating']\n",
    "    actual_rating_change = actual_next_rating - current_rating\n",
    "    next_state = (\n",
    "        actual_next_rating,\n",
    "        data.iloc[episode + 1]['Problem Tags'] if episode + 1 < len(data) else '',  # Ensure this is a string of tags\n",
    "        data.iloc[episode + 1]['Contest Duration'] if episode + 1 < len(data) else 0\n",
    "    )\n",
    "    return next_state, actual_rating_change\n",
    "\n",
    "def calculate_reward(action, actual_rating_change):\n",
    "    return -abs(action - actual_rating_change)\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv('./tourist_problem.csv')\n",
    "\n",
    "# Automatically determine duration bins based on unique values\n",
    "unique_durations = sorted(data['Contest Duration'].unique())\n",
    "duration_bins = np.array(unique_durations + [max(unique_durations) + 1])\n",
    "\n",
    "# Extract unique problem tags\n",
    "all_tags = '; '.join(data['Problem Tags'])\n",
    "tags_split = [tag.strip() for sublist in all_tags.split(';') for tag in sublist.split(',') if tag.strip()]\n",
    "unique_tags = set(tags_split)\n",
    "print(f\"Problem Categories: {len(unique_tags)}\")\n",
    "# Limit to first 20 unique tags\n",
    "limited_tags = list(unique_tags)[:20]\n",
    "num_problem_categories = len(limited_tags)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data = data.iloc[:int(0.8 * len(data))]\n",
    "test_data = data.iloc[int(0.8 * len(data)):]\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.95\n",
    "epsilon = 1.0\n",
    "min_epsilon = 0.01\n",
    "decay_rate = 0.995\n",
    "num_episodes = len(train_data)\n",
    "max_steps_per_episode = 100\n",
    "\n",
    "rating_bins = [12]  # 12 bins for ratings\n",
    "problem_category_bins = [2] * num_problem_categories\n",
    "\n",
    "action_values = np.arange(-200, 201, 10)  # Actions from -200 to +200 in steps of 10\n",
    "action_bins = len(action_values)  # This should be the number of actions\n",
    "action_to_index = {v: i for i, v in enumerate(action_values)}\n",
    "\n",
    "# Instantiate the QLearningAgent\n",
    "q_learning_agent = QLearningAgent(alpha, gamma, epsilon, rating_bins + problem_category_bins + [len(duration_bins)], action_bins)\n",
    "q_learning_agent.duration_bins = duration_bins  # Set duration bins\n",
    "\n",
    "epochs = 100  # Number of times to iterate over the training dataset\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    # Training loop\n",
    "    for episode in range(num_episodes):\n",
    "        state = get_next_contest_state(train_data, episode)\n",
    "        state = q_learning_agent.discretize_state(state)\n",
    "        done = False\n",
    "        step = 0\n",
    "\n",
    "        while not done and step < max_steps_per_episode:\n",
    "            action_index = q_learning_agent.choose_action(state)\n",
    "            # Convert action index to value\n",
    "            action_value = action_values[action_index]\n",
    "\n",
    "            next_state, actual_rating_change = simulate_next_contest(train_data, episode, action_value)\n",
    "            next_state = q_learning_agent.discretize_state(next_state)\n",
    "            reward = calculate_reward(action_value, actual_rating_change)\n",
    "            q_learning_agent.update(state, action_value, reward, next_state, done)\n",
    "            state = next_state\n",
    "            step += 1\n",
    "            if episode + 1 == len(train_data):\n",
    "                done = True\n",
    "                \n",
    "        if epsilon > min_epsilon:\n",
    "            epsilon *= decay_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on Test Data: 16387.607843137255\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Evaluation on test data and calculate MSE\n",
    "test_predictions = []\n",
    "test_actuals = []\n",
    "episodes = []\n",
    "\n",
    "for episode in range(len(test_data)):\n",
    "    episodes.append(episode)\n",
    "    state = get_next_contest_state(test_data, episode)\n",
    "    state = q_learning_agent.discretize_state(state)\n",
    "    action_index = q_learning_agent.choose_action(state)\n",
    "    action_value = action_values[action_index]\n",
    "\n",
    "    predicted_rating_change = action_value\n",
    "    actual_next_rating = test_data.iloc[episode]['New Rating']\n",
    "    predicted_next_rating = test_data.iloc[episode]['Old Rating'] + predicted_rating_change\n",
    "\n",
    "    test_predictions.append(predicted_next_rating)\n",
    "    test_actuals.append(actual_next_rating)\n",
    "\n",
    "mse = np.mean((np.array(test_predictions) - np.array(test_actuals)) ** 2)\n",
    "print(f\"Mean Squared Error on Test Data: {mse}\")\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(episodes, test_actuals, label='Actual Ratings', color='blue', marker='o')\n",
    "plt.plot(episodes, test_predictions, label='Predicted Ratings', color='red', linestyle='dashed', marker='x')\n",
    "plt.title('Actual vs Predicted Ratings - Q Learning')\n",
    "plt.xlabel('Contest Number')\n",
    "plt.ylabel('Rating')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "# Save the figure\n",
    "plt.savefig('actual_vs_predicted_ratings_q_learning.png', dpi=1200)  # Adjust the file name and dpi as needed\n",
    "plt.savefig('actual_vs_predicted_ratings_q_learning.pdf', dpi=1200)  # Adjust the file name and dpi as needed\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tasnimllm]",
   "language": "python",
   "name": "conda-env-tasnimllm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
