{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem Categories: 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 203/203 [03:25<00:00,  1.01s/it]\n",
      "100%|██████████| 203/203 [04:04<00:00,  1.21s/it]\n",
      "100%|██████████| 203/203 [04:31<00:00,  1.34s/it]\n",
      "100%|██████████| 203/203 [02:31<00:00,  1.34it/s]\n",
      "100%|██████████| 203/203 [02:37<00:00,  1.29it/s]\n",
      "100%|██████████| 203/203 [02:49<00:00,  1.20it/s]\n",
      "100%|██████████| 203/203 [02:29<00:00,  1.36it/s]\n",
      "100%|██████████| 203/203 [02:27<00:00,  1.38it/s]\n",
      "100%|██████████| 203/203 [02:25<00:00,  1.39it/s]\n",
      "100%|██████████| 203/203 [02:25<00:00,  1.40it/s]\n",
      "100%|██████████| 10/10 [29:47<00:00, 178.77s/it]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# Check if GPU is available and set the device accordingly\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "class DQNNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size=256):\n",
    "        super(DQNNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.ln1 = nn.LayerNorm(hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.ln2 = nn.LayerNorm(hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.ln3 = nn.LayerNorm(hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, action_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.leaky_relu(self.ln1(self.fc1(state)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.leaky_relu(self.ln2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.leaky_relu(self.ln3(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        q_values = self.fc4(x)\n",
    "        return q_values\n",
    "\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, learning_rate, gamma, epsilon, min_epsilon, decay_rate):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = gamma  # Discount rate\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.epsilon_decay = decay_rate\n",
    "        self.model = DQNNetwork(state_size, action_size).to(device)\n",
    "        self.target_network = DQNNetwork(state_size, action_size).to(device)\n",
    "        self.target_network.load_state_dict(self.model.state_dict())\n",
    "        self.target_network.eval()  # Set the target network to evaluation mode\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)  # Send to device\n",
    "        with torch.no_grad():\n",
    "            action_values = self.model(state)\n",
    "        return np.argmax(action_values.cpu().data.numpy())  # Move data back to CPU for numpy operations\n",
    "\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            state = torch.FloatTensor(state).unsqueeze(0).to(device)  # Send to device\n",
    "            next_state = torch.FloatTensor(next_state).unsqueeze(0).to(device)  # Send to device\n",
    "\n",
    "            action = torch.LongTensor([action]).to(device)\n",
    "            reward = torch.FloatTensor([reward]).to(device)\n",
    "            done = torch.FloatTensor([done]).to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            Q_values = self.model(state)\n",
    "            Q_expected = Q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "            Q_next = self.model(next_state).max(1)[0].detach()\n",
    "            Q_target = reward + (self.gamma * Q_next * (1 - done))\n",
    "\n",
    "            loss = nn.MSELoss()(Q_expected, Q_target)\n",
    "\n",
    "            # Backward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        if self.epsilon > self.min_epsilon:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv('tourist_problem.csv')\n",
    "\n",
    "# Extract unique problem tags\n",
    "all_tags = '; '.join(data['Problem Tags'])\n",
    "tags_split = [tag.strip() for sublist in all_tags.split(';') for tag in sublist.split(',') if tag.strip()]\n",
    "unique_tags = set(tags_split)\n",
    "print(f\"Problem Categories: {len(unique_tags)}\")\n",
    "# Limit to first -1 unique tags\n",
    "limited_tags = list(unique_tags)\n",
    "num_problem_categories = len(limited_tags)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data = data.iloc[:int(0.8 * len(data))]\n",
    "test_data = data.iloc[int(0.8 * len(data)):]\n",
    "\n",
    "# Define state size based on rating index, problem categories, and duration index\n",
    "rating_bins_edges = [0, 1000, 1200, 1400, 1600, 1800, 2000, 2200, 2400, 2700, 3000, 3001]\n",
    "unique_durations = sorted(data['Contest Duration'].unique())\n",
    "duration_bins = np.array(unique_durations + [max(unique_durations) + 1])\n",
    "state_size = len(rating_bins_edges) - 1 + num_problem_categories + len(duration_bins)\n",
    "\n",
    "# print(len(rating_bins_edges)-1, num_problem_categories, len(duration_bins))\n",
    "\n",
    "action_values = np.arange(-200, 201, 10)  # Actions from -200 to +200 in steps of 10\n",
    "action_bins = len(action_values)  # Number of actions\n",
    "action_size = len(action_values)  # Define action size\n",
    "action_to_index = {v: i for i, v in enumerate(action_values)}\n",
    "\n",
    "# print(\"Calculated State Size:\", state_size)\n",
    "\n",
    "agent = DQNAgent(state_size, action_size, 0.001, 0.95, 1.0, 0.01, 0.995)\n",
    "\n",
    "\n",
    "\n",
    "# Functions to process data and simulate environment\n",
    "def discretize_state(state):\n",
    "    rating_idx = np.digitize(state[0], bins=rating_bins_edges) - 1\n",
    "\n",
    "    # Handle edge cases where rating_idx might be outside the expected range\n",
    "    rating_idx = max(0, min(rating_idx, len(rating_bins_edges) - 2))\n",
    "\n",
    "    rating_one_hot = [0] * (len(rating_bins_edges) - 1)\n",
    "    rating_one_hot[rating_idx] = 1\n",
    "\n",
    "    # Process problem categories\n",
    "    problem_categories = [0] * num_problem_categories\n",
    "    tags_in_state = [tag.strip() for sublist in state[1].split(';') for tag in sublist.split(',') if tag.strip()]\n",
    "    for tag in tags_in_state:\n",
    "        if tag in limited_tags:\n",
    "            problem_categories[limited_tags.index(tag)] = 1\n",
    "\n",
    "    # One-hot encode duration index\n",
    "    duration_idx = np.digitize(state[2], bins=duration_bins) - 1\n",
    "    duration_one_hot = [0] * len(duration_bins)\n",
    "    duration_one_hot[duration_idx] = 1\n",
    "\n",
    "    # Combine all parts of the state\n",
    "    discretized_state = np.array(rating_one_hot + problem_categories + duration_one_hot)\n",
    "    return discretized_state\n",
    "\n",
    "\n",
    "\n",
    "def get_next_contest_state(data, episode):\n",
    "    contest_data = data.iloc[episode]\n",
    "    state = (contest_data['Old Rating'], contest_data['Problem Tags'], contest_data['Contest Duration'])\n",
    "    return state\n",
    "\n",
    "def simulate_next_contest(data, episode, action):\n",
    "    current_rating = data.iloc[episode]['Old Rating']\n",
    "    predicted_next_rating = current_rating + action\n",
    "    actual_next_rating = data.iloc[episode]['New Rating']\n",
    "    actual_rating_change = actual_next_rating - current_rating\n",
    "\n",
    "    # print(f\"Current Episode: {episode}, Current Rating: {current_rating}, Predicted Next Rating: {predicted_next_rating}, Actual Next Rating: {actual_next_rating}\")\n",
    "\n",
    "    if episode + 1 < len(data):\n",
    "        next_contest_data = data.iloc[episode + 1]\n",
    "        next_state = (\n",
    "            next_contest_data['Old Rating'],\n",
    "            next_contest_data['Problem Tags'],\n",
    "            next_contest_data['Contest Duration']\n",
    "        )\n",
    "    else:\n",
    "        next_state = (current_rating, '', 0)  # End of dataset, default state\n",
    "\n",
    "    # print(f\"Next State: {next_state}\")\n",
    "    return next_state, actual_rating_change\n",
    "\n",
    "def calculate_reward(action, actual_rating_change):\n",
    "    error = action - actual_rating_change\n",
    "    reward = -error ** 2\n",
    "    scaled_reward = reward / 1000  # Example scaling factor\n",
    "    return scaled_reward\n",
    "\n",
    "# Training Loop\n",
    "epochs = 10  # Number of times to iterate over the training dataset\n",
    "num_episodes = len(train_data)\n",
    "\n",
    "max_steps_per_episode = 10  # Define maximum steps per episode if not already defined\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    for episode in tqdm(range(num_episodes)):\n",
    "        state = get_next_contest_state(train_data, episode)\n",
    "        state = discretize_state(state)\n",
    "        # print(f\"Starting Episode {episode} with initial state: {state}\")\n",
    "        done = False\n",
    "        step = 0\n",
    "\n",
    "        while not done and step < max_steps_per_episode:\n",
    "            action_index = agent.choose_action(state)\n",
    "            action_value = action_values[action_index]\n",
    "            # print(f\"Episode {episode}, Step {step}, Current State: {state}, Action Index: {action_index}, Action Value: {action_value}\")\n",
    "\n",
    "            next_state, actual_rating_change = simulate_next_contest(train_data, episode, action_value)\n",
    "            next_state = discretize_state(next_state)\n",
    "\n",
    "            reward = calculate_reward(action_value, actual_rating_change)\n",
    "            # print(f\"Predicted Next State: {next_state}, Actual Rating Change: {actual_rating_change}, Reward: {reward}\")\n",
    "\n",
    "            # if np.array_equal(state, next_state):\n",
    "            #     print(\"No state change detected.\")\n",
    "\n",
    "            state = next_state  # Update the state\n",
    "            agent.remember(state, action_index, reward, next_state, done)\n",
    "            step += 1\n",
    "\n",
    "            if episode + 1 == len(train_data) or step >= max_steps_per_episode:\n",
    "                done = True\n",
    "                # print(f\"Episode {episode} finished at step {step}\")\n",
    "\n",
    "        agent.replay(32)  # Replay with batch size 32\n",
    "        agent.update_target_network()  # Update target network\n",
    "\n",
    "        # Monitor epsilon value after decay\n",
    "        # print(f\"Epsilon after decay: {agent.epsilon}\")\n",
    "        # print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "agent.model.eval()\n",
    "\n",
    "# Check if GPU is available and set the device accordingly\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming test_data is a DataFrame similar to train_data\n",
    "def evaluate_model(agent, test_data, action_values):\n",
    "    test_predictions = []\n",
    "    test_actuals = []\n",
    "    episodes = []\n",
    "\n",
    "    for episode in range(len(test_data)):\n",
    "        episodes.append(episode)\n",
    "        state = get_next_contest_state(test_data, episode)\n",
    "        # Ensure that state is based only on information up to the contest, not after\n",
    "        state = discretize_state(state)\n",
    "        \n",
    "        # Convert state to PyTorch tensor and send to the device\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            q_values = agent.model(state_tensor)\n",
    "        action_index = q_values.max(1)[1].item()  # Get the index of the max q-value\n",
    "        action_value = action_values[action_index]\n",
    "\n",
    "        # Apply the predicted rating change to the 'Old Rating'\n",
    "        predicted_rating_change = action_value\n",
    "        actual_next_rating = test_data.iloc[episode]['New Rating']  # This is the true rating after the contest\n",
    "        predicted_next_rating = test_data.iloc[episode]['Old Rating'] + predicted_rating_change  # Prediction\n",
    "\n",
    "        test_predictions.append(predicted_next_rating)\n",
    "        test_actuals.append(actual_next_rating)\n",
    "\n",
    "    mse = np.mean((np.array(test_predictions) - np.array(test_actuals)) ** 2)\n",
    "    print(f\"Mean Squared Error on Test Data: {mse}\")\n",
    "\n",
    "    print(f\"Actual Rating: {test_actuals}\")\n",
    "    print(f\"Predicted Rating: {test_predictions}\")\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(episodes, test_actuals, label='Actual Ratings', color='blue', marker='o')\n",
    "    plt.plot(episodes, test_predictions, label='Predicted Ratings', color='red', linestyle='dashed', marker='x')\n",
    "    plt.title('Actual vs Predicted Ratings')\n",
    "    plt.xlabel('Contest Number')\n",
    "    plt.ylabel('Rating')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Call the evaluate function\n",
    "evaluate_model(agent, test_data, action_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
